{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Copyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT License."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Creating and Updating a Docker Image before Deployment as a Webservice\n\nThis notebook demonstrates how to make changes to an existing docker image, before deploying it as a webservice.  \n\nKnowing how to do this can be helpful, for example if you need to debug the execution script of a webservice you're developing, and debugging it involves several iterations of code changes.  In this case it is not an option to deploy your application as a webservice at every iteration, because the time it takes to deploy your service will significantly slow you down.  In some cases, it may be easier to simply run the execution script on the command line, but this not an option if your script accumulates data across individual calls.\n\nWe describe the following process:\n\n1. Configure your Azure Workspace.\n2. Create a Docker image, using the Azure ML SDK.\n3. Test your Application by running the Docker container locally.\n4. Update the execution script inside your running Docker container.\n5. Commit the changes in your Docker container to its image\n6. Update the image in the Azure Container Registry (ACR).\n7. Deploy your Docker image as an Azure Container Instance ([ACI](https://azure.microsoft.com/en-us/services/container-instances/)) Webservice.\n8. Test your Webservice.\n    \n> Several cells below are completely commented out. This is because they can only be run on Jupyter, but not on Azure Databricks.  If you do have access to Jupyter, we recommend to explore these cells, because they give you an insight into how to debug a docker container."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Prerequisites\n- You need to have an [Azure](https://azure.microsoft.com) subscription. You will also need to know a subscription_id and resource_group.  One way to discover those is to visit the [azure portal](https://portal.azure.com) and look use the same as those of the DSVM.\n\n**Note:** \n- This code was tested on a Data Science Virtual Machine ([DSVM](https://azure.microsoft.com/en-us/services/virtual-machines/data-science-virtual-machines/)), running Ubuntu Linux 16.04 (Xenial). **Do *not* try to run this notebook on *Databricks***, because you may run into trouble executing some of the shell and docker commands.\n- If you get an error message when trying to import `azureml` in the first cell below, you probably have to switch to using the correct kernel: `Python [conda env:amladpm]`."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Configure your Azure Workspace\n\nWe start by selecting your workspace, and make sure we have access to it from here.  In order for this to work, make sure you followed the instructions for creating a workspace in your development environment:\n\n- [DSVM](../lab0.0_Setting_Up_Env/configure_environment_DSVM.ipynb)\n- [Aure Databricks](../lab0.0_Setting_Up_Env/configure_environment_ADB.ipynb)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# %matplotlib inline  \n\nimport os\nfrom azureml.core import Workspace\nimport pandas as pd\nimport urllib\n\nconfig_path = '..'\n\n# # run this if you are using Jupyter (instead of azure datarbicks)\n# config_path = os.path.expanduser('~')\n\nws = Workspace.from_config(path=os.path.join(config_path, 'aml_config', 'config.json'))\n\nws.get_details()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's make sure that you have the correction version of the Azure ML SDK installed on your workstation or VM.  If you don't have the write version, please follow these [Installation Instructions](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-create-workspace-with-python#install-the-sdk)."
    },
    {
      "metadata": {
        "tags": [
          "check version"
        ],
        "trusted": true
      },
      "cell_type": "code",
      "source": "import azureml\n\n# display the core SDK version number\nprint(\"Azure ML SDK Version: \", azureml.core.VERSION)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Create a Docker image using the Azure ML SDK"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create a template execution script for your application\n\nWe are going to start with just a barebones execution script for your webservice.  This script will calculate the running average of numbers thrown at it.\n\nWe recommend that you execute the cells for your `score.py` scripts twice.\n\n1. With a `#` sign at the beginning of the first line. This way you can detect typos and syntax errors during execution.\n2. If the script runs OK, you can remove the `#` sign, to write the script to a file instead of executing it."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# #%%writefile score.py\n\n# import json # we use json in order to interact with the anomaly detection service via a RESTful API\n\n# # The init function is only run once, when the webservice (or Docker container) is started\n# def init():\n#     global running_avg, curr_n\n    \n#     running_avg = 0.0\n#     curr_n = 0\n    \n#     pass\n\n# # the run function is run everytime we interact with the service\n# def run(raw_data):\n#     \"\"\"\n#     Calculates rolling average according to Welford's online algorithm.\n#     https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online\n\n#     :param raw_data: raw_data should be a json query containing a dictionary with the key 'value'\n#     :return: runnin_avg (float, json response)\n#     \"\"\"\n#     global running_avg, curr_n\n    \n#     value = json.loads(raw_data)['value']\n#     n_arg = 5 # we calculate the average over the last \"n\" measures\n    \n#     curr_n += 1\n#     n = min(curr_n, n_arg) # in case we don't have \"n\" measures yet\n    \n#     running_avg += (value - running_avg) / n\n    \n#     return json.dumps(running_avg)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create environment file for your Conda environment\n\nNext, create an environment file (environment.yml) that specifies all the python dependencies of your script. This file is used to ensure that all of those dependencies are installed in the Docker image.  Let's assume your Webservice will require ``azureml-sdk``, ``scikit-learn``, and ``pynacl``."
    },
    {
      "metadata": {
        "tags": [
          "set conda dependencies"
        ],
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.conda_dependencies import CondaDependencies \n\nmyenv = CondaDependencies()\nmyenv.add_conda_package(\"scikit-learn\")\nmyenv.add_pip_package(\"pynacl==1.2.1\")\nmyenv.add_pip_package(\"pyculiarity\")\nmyenv.add_pip_package(\"pandas\")\nmyenv.add_pip_package(\"numpy\")\n\nwith open(\"environment.yml\",\"w\") as f:\n    f.write(myenv.serialize_to_string())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Review the content of the `environment.yml` file."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "with open(\"environment.yml\",\"r\") as f:\n    print(f.read())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create the initial Docker image\n\nThe next step is to create a docker image.  \n\nWe start by creating downloading a scoring script that we prepared for this course.  You can skip the details of this script, if you are in a hurry.  Briefly, it contains our solution to online anomaly detection from the previous lab."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# uncomment the below lines to see the solution\nfilename = 'AD_score.py'\nurllib.request.urlretrieve(\n    os.path.join('https://raw.githubusercontent.com/Azure/LearnAI-ADPM/master/solutions/', filename),\n    filename='score.py')\n\nwith open('score.py') as f:\n    print(f.read())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%time\n\nfrom azureml.core.image import ContainerImage\n\n# configure the image\nimage_config = ContainerImage.image_configuration(execution_script = \"score.py\", \n                                                  runtime = \"python\",\n                                                  conda_file = \"environment.yml\")\n\n# create the docker image. this should take less than 5 minutes\nimage = ContainerImage.create(name = \"my-docker-image\",\n                              image_config = image_config,\n                              models = [],\n                              workspace = ws)\n\n# we wait until the image has been created\nimage.wait_for_creation(show_output=True)\n\n# let's save the image location\nimageLocation = image.serialize()['imageLocation']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Test your Application by running the Docker container locally\n\n### Download the created Docker image from the Azure Container Registry ([ACR](https://azure.microsoft.com/en-us/services/container-registry/))\n\nHere we use some [cell magic](https://ipython.readthedocs.io/en/stable/interactive/magics.html) to exchange variables between python and bash."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# %%bash -s \"$imageLocation\" \n\n# # get the location of the docker image in ACR\n# imageLocation=$1\n\n# # extract the address of the repository within ACR\n# repository=$(echo $imageLocation | cut -f 1 -d \".\")\n\n# echo \"Attempting to login to repository $repository\"\n# az acr login --name $repository\n# echo\n\n# echo \"Trying to pull image $imageLocation\"\n# docker pull $imageLocation",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Start the docker container\n\nWe use standard Docker commands to start the container locally."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# %%bash -s \"$imageLocation\"\n\n# # extract image name and tag from imageLocation\n# image_name=$(echo $1 | cut -f 1 -d \":\")\n# tag=$(echo $1 | cut -f 2 -d \":\")\n# echo \"Image name: $image_name, tag: $tag\"\n\n# # extract image ID from list of downloaded docker images\n# image_id=$(docker images $image_name:$tag --format \"{{.ID}}\")\n# echo \"Image ID: $image_id\"\n\n# # we forward TCP port 5001 of the docker container to local port 8080 for testing\n# echo \"Starting docker container\"\n# docker run -d -p 8888:5001 $image_id\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Test the docker container\n\nWe test the docker container, by sending some data to it to see how it responds - just as we would with a Webservice.\n\n> If you get an error message below, you may just have to wait a couple of seconds."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# import json\n# import requests\n# import numpy as np\n# import matplotlib.pyplot as plt\n\n# values = np.random.normal(0,1,100)\n# values = np.cumsum(values)\n\n\n# running_avgs = []\n\n# for value in values:\n#     raw_data = {\"value\": value}\n\n#     r = requests.post('http://localhost:8888/score', json=raw_data)\n\n#     result = json.loads(r.json())\n#     running_avgs.append(result)\n\n# plt.close()\n# plt.plot(values)\n# plt.plot(running_avgs)\n# display()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Modifying the container\n\nLet's make a change to the the execution script: Copy the changed ``AD_score.py`` into the running docker container and commit the changes to the container image."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# %%bash -s $imageLocation\n\n# image_location=$1\n\n# # extract image name and tag from imageLocation\n# image_name=$(echo $image_location | cut -f 1 -d \":\")\n# tag=$(echo $image_location | cut -f 2 -d \":\")\n\n# echo \"Image name: $image_name, tag: $tag\"\n\n# # extract image id\n# image_id=$(docker images $image_name:$tag --format \"{{.ID}}\")\n\n# echo \"Image ID: $image_id\"\n\n# # extract container ID\n# container_id=$(docker ps | tail -n1 | cut -f 1 -d \" \")\n# echo \"Container ID: $container_id\"\n\n# # copy modified scoring script again\n# docker cp AD_score.py $container_id:/var/azureml-app/score.py\n# sleep 1\n\n# # stop the container\n# docker restart $container_id",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Test the container"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Load telemetry data"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "base_path = 'https://sethmottstore.blob.core.windows.net'\ndata_dir = os.path.join(base_path, 'predmaint')\n\nprint(\"Reading data ... \", end=\"\")\ntelemetry = pd.read_csv(os.path.join(data_dir, 'telemetry.csv'))\nprint(\"Done.\")\n\nprint(\"Parsing datetime...\", end=\"\")\ntelemetry['datetime'] = pd.to_datetime(telemetry['datetime'], format=\"%m/%d/%Y %I:%M:%S %p\")\ntelemetry.columns = ['timestamp', 'machineID', 'volt', 'rotate', 'pressure', 'vibration']\nprint(\"Done.\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# import numpy as np\n# import json\n# import requests\n\n# def test_docker(telemetry, n=None):\n#     \"\"\"\n#         n is the number of sensor readings we are simulating\n#         \"\"\"\n\n#     if not n:\n#         n = telemetry.shape[0]\n\n#     machine_ids = [1] # telemetry['machineID'].unique()\n#     timestamps = telemetry['timestamp'].unique()\n\n#     out_df = pd.DataFrame()\n#     for timestamp in timestamps[:10]:\n#         np.random.shuffle(machine_ids)\n#         for machine_id in machine_ids:\n#             data = telemetry.loc[(telemetry['timestamp'] == timestamp) & (telemetry['machineID'] == machine_id), :]\n#             json_data = data.to_json()\n#             input_data = {\"data\": json_data}\n           \n#             r = requests.post('http://localhost:8888/score', json=input_data)\n\n#             result = pd.read_json(json.loads(r.json()))\n#             out_df = out_df.append(result, ignore_index=True)\n#     return out_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# print(\"Processing ... \")\n# out_df = test_docker(telemetry)\n# print(\"Done.\")\n# out_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Push the updated container to ACR\n\n**First**, test your Docker container again (run the json query above), to ensure that the changes are having the expected effect. \n\n**Then** you can push the image into ACR, so that it can be retrieved by the Azure ML SDK when you want to deploy your Webservice."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# %%bash -s \"$imageLocation\"\n\n# image_location=$1\n\n# # extract container ID\n# container_id=$(docker ps | tail -n1 | cut -f 1 -d \" \")\n# echo \"Container ID: $container_id\"\n\n# # commit changes made in the container to the local copy of the image\n# docker commit $container_id $image_location\n\n# docker push $image_location",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's try to deploy the container to ACI, just to make sure everything behaves as expected."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%time\nfrom azureml.core.webservice import Webservice\nfrom azureml.core.image import ContainerImage\nfrom azureml.core.webservice import AciWebservice\n\n# create configuration for ACI\naciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n                                               memory_gb=1, \n                                               tags={\"data\": \"some data\",  \"method\" : \"machine learning\"}, \n                                               description=\"Does machine learning on some data\")\n# pull the image\nimage = ContainerImage(ws, name='my-docker-image')\n\n# deploy webservice\nservice_name = 'my-web-service'\nservice = Webservice.deploy_from_image(deployment_config = aciconfig,\n                                            image = image,\n                                            name = service_name,\n                                            workspace = ws)\nservice.wait_for_deployment(show_output = True)\nprint(service.state)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport json\nimport requests\n\ndef test_webservice(telemetry, n=None):\n    \"\"\"\n        n is the number of sensor readings we are simulating\n        \"\"\"\n\n    if not n:\n        n = telemetry.shape[0]\n\n    machine_ids = [1] # telemetry['machineID'].unique()\n    timestamps = telemetry['timestamp'].unique()\n\n    out_df = pd.DataFrame()\n    for timestamp in timestamps[:n]:\n        np.random.shuffle(machine_ids)\n        for machine_id in machine_ids:\n            data = telemetry.loc[(telemetry['timestamp'] == timestamp) & (telemetry['machineID'] == machine_id), :]\n            json_data = data.to_json()\n            input_data = bytes(json.dumps({\"data\": json_data}), encoding = 'utf8')\n    \n            result = pd.read_json(json.loads(service.run(input_data=input_data)))\n\n            out_df = out_df.append(result, ignore_index=True)\n    return out_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Processing ... \")\nout_df = test_webservice(telemetry, n=10)\nprint(\"Done.\")\nout_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "service.serialize()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Clean up resources\n\nTo keep the resource group and workspace for other tutorials and exploration, you can delete only the ACI deployment using this API call:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "service.delete()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# The end"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Copyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT License."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "msauthor": "sgilley",
    "name": "AD_model_deployment",
    "notebookId": 1951564739234124
  },
  "nbformat": 4,
  "nbformat_minor": 1
}