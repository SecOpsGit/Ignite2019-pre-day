{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Copyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT License."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Data Exploration\n\nIn this lab, we will explore and visualize our telemetry data.  You will learn how calculate metrics on top of your raw time series to gain deeper insights into your data.  \n\nIn this lab, you will:\n- Get to know your dataset better by visualizing it\n- Learn how to visualize time series data\n- Become familiar with a set of standard metrics that can be defined on time series data\n- Understand when to use which metric"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Load and visualize/explore your data"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# %matplotlib inline \n\n# let's set up your environment, and define some global variables\n\nimport os\nfrom rstl import STL\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nimport seaborn as sns\nimport numpy as np\n\n\n\n# adjust this based on your screen's resolution\nfig_panel = (18, 16)\nwide_fig = (16, 4)\ndpi=80 ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# next, we load the telemetry data\n\nbase_path = 'https://sethmottstore.blob.core.windows.net'\ndata_subdir = 'predmaint'\ndata_filename = 'telemetry.csv'\ndata_path = os.path.join(base_path, data_subdir, data_filename)\n\nprint(\"Reading data ... \", end=\"\")\ndf = pd.read_csv(data_path)\nprint(\"Done.\")\n\nprint(\"Parsing datetime...\", end=\"\")\ndf['datetime'] = pd.to_datetime(df['datetime'], format=\"%m/%d/%Y %I:%M:%S %p\")\nprint(\"Done.\")\n\ndf = df.rename(str, columns={'datetime': 'timestamp'})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# let's define some useful variables\nsensors = df.columns[2:].tolist() # a list containing the names of the sensors\nmachines = df['machineID'].unique().tolist() # a list of our machine ids\n\nn_sensors = len(sensors)\nn_machines = len(machines)\nprint(\"We have %d sensors: %s for each of %d machines.\" % (n_sensors, sensors, n_machines))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# let's pick a random machine\nrandom_machine = 67\n\ndf_s = df.loc[df['machineID'] == random_machine, :]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# let's get some info about the time domain\ndf_s['timestamp'].describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Question**: At which frequency do we receive sensor data?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# create a table of descriptive statistics for our data set\ndf_s.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's do some time series specific exploration of the data"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "n_samples = 24*14 # we look at the first 14 days of sensor data\n\n\nplt.close()\nfig, ax = plt.subplots(2, 2, figsize=fig_panel, dpi=dpi) # create 2x2 panel of figures\nfor s, sensor in enumerate(sensors):\n    c = s%2 # column of figure panel\n    r = int(s/2) # row of figure panel\n    ax[r,c].plot(df_s['timestamp'][:n_samples], df_s[sensor][:n_samples])\n    ax[r,c].set_title(sensor)\ndisplay()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, we create histogram plots to have an understanding of how these data are distributed."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "n_bins=200\n\nplt.close()\nfig, ax = plt.subplots(2,2,figsize=fig_panel, dpi=dpi)\nfor s, sensor in enumerate(sensors):\n    c = s%2\n    r = int(s/2)\n    sns.distplot(df_s[sensor], ax=ax[r,c])\ndisplay()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Useful metrics for time series data\n\n### Bollinger Bands\n\n[Bollinger Bands](https://en.wikipedia.org/wiki/Bollinger_Bands) are a type of statistical chart characterizing the prices and volatility over time of a financial instrument or commodity, using a formulaic method propounded by John Bollinger in the 1980s. Financial traders employ these charts as a methodical tool to inform trading decisions, control automated trading systems, or as a component of technical analysis. \n\nThis can be done very quickly with pandas, because it has a built-in function `ewm` for convolving the data with a sliding window with exponential decay, which can be combined with standard statistical functions, such as `mean` or `std`.\n\nOf course, you can imagine that rolling means, standard deviations etc can be useful on their own, without using them for creating Bollinger Bands."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "window_size = 12 # the size of the window over which to aggregate\nsample_size = 24 * 7 * 2 # let's only look at two weeks of data\nx = df_s['timestamp']\n\n\nplt.close()\nfig, ax = plt.subplots(2, 2, figsize=fig_panel, dpi=dpi)\nfor s, sensor in enumerate(sensors):\n    c = s%2\n    r = int(s/2)\n    rstd = df_s[sensor].ewm(window_size).std()\n    rm = df_s[sensor].ewm(window_size).mean()\n    ax[r,c].plot(x[window_size:sample_size], df_s[sensor][window_size:sample_size], color='blue', alpha=.2)\n    ax[r,c].plot(x[window_size:sample_size], rm[window_size:sample_size] - 2 * rstd[window_size:sample_size], color='grey')\n    ax[r,c].plot(x[window_size:sample_size], rm[window_size:sample_size] + 2 * rstd[window_size:sample_size], color='grey')\n    ax[r,c].plot(x[window_size:sample_size], rm[window_size:sample_size], color='black')\n    ax[r,c].set_title(sensor)\ndisplay()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Lag features\n\nLag features can be very useful in machine learning approaches dealing with time series.  For example, if you want to train a model to predict whether a machine is going to fail the next day, you can just shift your logs of failures forward by a day, so that failures (i.e. target labels) are aligned with the feature data you will use for predicting failures.\n\nLuckily, pandas has a built-in `shift` function for doing this."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "sample_size = 24 * 2 # let's only look at first two days\nx = df_s['timestamp']\n\nplt.close()\nfig, ax = plt.subplots(2, 2, figsize=fig_panel, dpi=dpi)\nfor s, sensor in enumerate(sensors):\n    c = s%2\n    r = int(s/2)\n    rstd = df_s[sensor].ewm(window_size).std()\n    rm = df_s[sensor].ewm(window_size).mean()\n    ax[r,c].plot(x[:sample_size], df_s[sensor][:sample_size], color='black', alpha=1, label='orig')\n    ax[r,c].plot(x[:sample_size], df_s[sensor][:sample_size].shift(-1), color='blue', alpha=1, label='-1h') # shift by x hour\n    ax[r,c].plot(x[:sample_size], df_s[sensor][:sample_size].shift(-2), color='blue', alpha=.5, label='-2h') # shift by x hour\n    ax[r,c].plot(x[:sample_size], df_s[sensor][:sample_size].shift(-3), color='blue', alpha=.2, label='-3h') # shift by x hour\n    ax[r,c].set_title(sensor)\nax[r,c].legend()\ndisplay()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Rolling entropy\n\nDepending on your use-case entropy can also be a useful metric, as it gives you an idea of how evenly your measures are distributed in a specific range. For more information, visit Wikipedia:\n\nhttps://en.wikipedia.org/wiki/Entropy_(information_theory)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from scipy.stats import entropy\n\nsample_size = 24*7*4 # use the first x hours of data\n\nsensor = 'volt'\nsensor_data = df_s[sensor]\nrolling_entropy = sensor_data.rolling(12).apply(entropy)\n\nplt.close()\nfig, ax = plt.subplots(2,1, figsize=wide_fig)\nax[0].plot(x[:sample_size], sensor_data[:sample_size])\nax[1].plot(x[:sample_size], rolling_entropy[:sample_size])\ndisplay()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Other useful metrics\n\nThere are various other useful metrics for timeseries data.  You may keep them in the back of your mind when you are dealing with another scenario.\n\n- Rolling median, min, max, mode etc. statistics\n- Rolling majority, for categorical features\n- Rolling text statistics for text features\n- [Short-time fourier transform](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.stft.html)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Quiz\n\nThe big question is when to use which metric for your use-case.  \n\nHere are a couple of sample scenarios. Can you recommend which one of the above metrics to use in each case?\n1. You are developing a fitness application for mobile phones that have an [accelerometer](https://en.wikipedia.org/wiki/Accelerometer). You want to be able to measure how much time a user spends sitting, walking, and running over the course of a day. Which metric would you use to identify the different activities?\n2. You want to get rich on the stock market, but you hate volatility.  Which metric would you use to measure volatility?\n3. You are in charge of a server farm.  You are looking for a way to detect denial of service attacks on your servers.  You don't want to constantly look at individual amounts of traffic at all of the servers at the same time.  However, you know that all of the servers typically get a constant amount of traffic.  Which metric could you use to determine that things have shifted, such as when some servers seem to be getting a lot more traffic than the other servers?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Copyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT License."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "name": "02_AD_data_preparation_for_time_series",
    "notebookId": 4063271094430139
  },
  "nbformat": 4,
  "nbformat_minor": 1
}