{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and Updating a Docker Image before Deployment as a Webservice\n",
    "\n",
    "This notebook demonstrates how to make changes to an existing docker image, before deploying it as a webservice.  \n",
    "\n",
    "Knowing how to do this can be helpful, for example if you need to debug the execution script of a webservice you're developing, and debugging it involves several iterations of code changes.  In this case it is not an option to deploy your application as a webservice at every iteration, because the time it takes to deploy your service will significantly slow you down.  In some cases, it may be easier to simply run the execution script on the command line, but this not an option if your script accumulates data across individual calls.\n",
    "\n",
    "We describe the following process:\n",
    "\n",
    "1. Configure your Azure Workspace.\n",
    "2. Create a Docker image, using the Azure ML SDK.\n",
    "3. Test your Application by running the Docker container locally.\n",
    "4. Update the execution script inside your running Docker container.\n",
    "5. Commit the changes in your Docker container to the Docker image and update the Azure Container Registry (ACR).\n",
    "6. Deploy your Docker image as an Azure Container Instance ([ACI](https://azure.microsoft.com/en-us/services/container-instances/)) Webservice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "- You need to have an [Azure](https://azure.microsoft.com) subscription.\n",
    "\n",
    "**Note:** This code was tested on a Data Science Virtual Machine ([DSVM](https://azure.microsoft.com/en-us/services/virtual-machines/data-science-virtual-machines/)), running Ubuntu Linux 16.04 (Xenial)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure your Azure Workspace\n",
    "\n",
    "We need to set up your workspace, and make sure we have access to it from here.\n",
    "\n",
    "This requires that you have downloaded the ***config.json*** configuration file for your azure workspace.\n",
    "\n",
    "Follow this [Quickstart](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-get-started) to set up your workspace and to download the config.json file, which contains information about the workspace you just created. Save the file in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that you have the correction version of the Azure ML SDK installed on your workstation or VM.  If you don't have the write version, please follow these [Installation Instructions](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-create-workspace-with-python#install-the-sdk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "check version"
    ]
   },
   "outputs": [],
   "source": [
    "import azureml\n",
    "\n",
    "# display the core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)\n",
    "\n",
    "if azureml.core.VERSION == '0.1.59':\n",
    "    print(\"Looks like you have the correct version. We are good to go.\")\n",
    "else:\n",
    "    print(\"There is a version mismatch, this notebook may not work as expected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Docker image using the Azure ML SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a template execution script for your application\n",
    "\n",
    "We are going to start with just a barebones execution script for your webservice.  This script is only for testing, and will do nothing but thank us for interacting with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "\n",
    "import json # we use json in order to interact with the anomaly detection service via a RESTful API\n",
    "\n",
    "# The init function is only run once, when the webservice (or Docker container) is started\n",
    "def init():\n",
    "    global running_avg, curr_n\n",
    "    \n",
    "    running_avg = 0.0\n",
    "    curr_n = 0\n",
    "    \n",
    "    pass\n",
    "\n",
    "# the run function is run everytime we interact with the service\n",
    "def run(raw_data):\n",
    "    \"\"\"\n",
    "    Calculates rolling average according to Welford's online algorithm.\n",
    "    https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online\n",
    "\n",
    "    :param raw_data: raw_data should be a json query containing a dictionary with the key 'value'\n",
    "    :return: runnin_avg (float, json response)\n",
    "    \"\"\"\n",
    "    global running_avg, curr_n\n",
    "    \n",
    "    value = json.loads(raw_data)['value']\n",
    "    n_arg = 5 # we calculate the average over the last \"n\" measures\n",
    "    \n",
    "    curr_n += 1\n",
    "    n = min(curr_n, n_arg) # in case we don't have \"n\" measures yet\n",
    "    \n",
    "    running_avg += (value - running_avg) / n\n",
    "    \n",
    "    return json.dumps(running_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create environment file for your Conda environment\n",
    "\n",
    "Next, create an environment file (environment.yml) that specifies all the python dependencies of your script. This file is used to ensure that all of those dependencies are installed in the Docker image.  Let's assume your Webservice will require ``azureml-sdk``, ``scikit-learn``, and ``pynacl``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "set conda dependencies"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "myenv = CondaDependencies()\n",
    "myenv.add_conda_package(\"scikit-learn\")\n",
    "myenv.add_pip_package(\"pynacl==1.2.1\")\n",
    "myenv.add_pip_package(\"pyculiarity\")\n",
    "myenv.add_pip_package(\"pandas\")\n",
    "myenv.add_pip_package(\"numpy\")\n",
    "\n",
    "with open(\"environment.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the content of the `environment.yml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"environment.yml\",\"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the initial Docker image\n",
    "\n",
    "We use the ``environment.yml`` and ``score.py`` files from above, to create an initial Docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from azureml.core.image import ContainerImage\n",
    "\n",
    "# configure the image\n",
    "image_config = ContainerImage.image_configuration(execution_script = \"score.py\", \n",
    "                                                  runtime = \"python\",\n",
    "                                                  conda_file = \"environment.yml\")\n",
    "\n",
    "# create the docker image. this should take less than 5 minutes\n",
    "image = ContainerImage.create(name = \"my-docker-image\",\n",
    "                              image_config = image_config,\n",
    "                              models = [],\n",
    "                              workspace = ws)\n",
    "\n",
    "# we wait until the image has been created\n",
    "image.wait_for_creation(show_output=True)\n",
    "\n",
    "# let's save the image location\n",
    "imageLocation = image.serialize()['imageLocation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your Application by running the Docker container locally\n",
    "\n",
    "### Download the created Docker image from the Azure Container Registry ([ACR](https://azure.microsoft.com/en-us/services/container-registry/))\n",
    "\n",
    "Here we use some [cell magic](https://ipython.readthedocs.io/en/stable/interactive/magics.html) to exchange variables between python and bash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$imageLocation\" \n",
    "\n",
    "# get the location of the docker image in ACR\n",
    "imageLocation=$1\n",
    "\n",
    "# extract the address of the repository within ACR\n",
    "repository=$(echo $imageLocation | cut -f 1 -d \".\")\n",
    "\n",
    "echo \"Attempting to login to repository $repository\"\n",
    "az acr login --name $repository\n",
    "echo\n",
    "\n",
    "echo \"Trying to pull image $imageLocation\"\n",
    "docker pull $imageLocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the docker container\n",
    "\n",
    "We use standard Docker commands to start the container locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$imageLocation\"\n",
    "\n",
    "# extract image name and tag from imageLocation\n",
    "image_name=$(echo $1 | cut -f 1 -d \":\")\n",
    "tag=$(echo $1 | cut -f 2 -d \":\")\n",
    "echo \"Image name: $image_name, tag: $tag\"\n",
    "\n",
    "# extract image ID from list of downloaded docker images\n",
    "image_id=$(docker images $image_name:$tag --format \"{{.ID}}\")\n",
    "echo \"Image ID: $image_id\"\n",
    "\n",
    "# we forward TCP port 5001 of the docker container to local port 8080 for testing\n",
    "echo \"Starting docker container\"\n",
    "docker run -d -p 8080:5001 $image_id\n",
    "\n",
    "sleep 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the docker container\n",
    "\n",
    "We test the docker container, by sending some data to it to see how it responds - just as we would with a Webservice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "values = np.random.normal(0,1,100)\n",
    "values = np.cumsum(values)\n",
    "\n",
    "\n",
    "running_avgs = []\n",
    "\n",
    "for value in values:\n",
    "    raw_data = {\"value\": value}\n",
    "\n",
    "    r = requests.post('http://localhost:8080/score', json=raw_data)\n",
    "\n",
    "    result = json.loads(r.json())\n",
    "    running_avgs.append(result)\n",
    "\n",
    "plt.plot(values)\n",
    "plt.plot(running_avgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the container\n",
    "\n",
    "Let's make a change to the the execution script: We want to enable an additional input argument to ``score.py`` to set how many previous values to consider in the running average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile score.py\n",
    "\n",
    "# import json # we use json in order to interact with the anomaly detection service via a RESTful API\n",
    "\n",
    "# # The init function is only run once, when the webservice (or Docker container) is started\n",
    "# def init():\n",
    "#     global running_avg, curr_n\n",
    "    \n",
    "#     running_avg = 0.0\n",
    "#     curr_n = 0\n",
    "    \n",
    "#     pass\n",
    "\n",
    "# # the run function is run everytime we interact with the service\n",
    "# def run(raw_data):\n",
    "#     \"\"\"\n",
    "#     Calculates rolling average according to Welford's online algorithm.\n",
    "#     https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online\n",
    "\n",
    "#     :param raw_data: raw_data should be a json query containing a dictionary with the key 'value'\n",
    "#     :return: runnin_avg (float, json response)\n",
    "#     \"\"\"\n",
    "#     global running_avg, curr_n\n",
    "    \n",
    "#     value = json.loads(raw_data)['value']\n",
    "#     n_arg = json.loads(raw_data)['n'] # we calculate the average over the last \"n\" measures\n",
    "    \n",
    "#     curr_n += 1\n",
    "#     n = min(curr_n, n_arg) # in case we don't have \"n\" measures yet\n",
    "    \n",
    "#     running_avg += (value - running_avg) / n\n",
    "    \n",
    "#     return json.dumps(running_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update container image\n",
    "\n",
    "Copy the changed ``score.py`` into the running docker container and commit the changes to the container image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s $imageLocation\n",
    "\n",
    "image_location=$1\n",
    "\n",
    "# extract image name and tag from imageLocation\n",
    "image_name=$(echo $image_location | cut -f 1 -d \":\")\n",
    "tag=$(echo $image_location | cut -f 2 -d \":\")\n",
    "\n",
    "echo \"Image name: $image_name, tag: $tag\"\n",
    "\n",
    "# extract image id\n",
    "image_id=$(docker images $image_name:$tag --format \"{{.ID}}\")\n",
    "\n",
    "echo \"Image ID: $image_id\"\n",
    "\n",
    "# extract container ID\n",
    "container_id=$(docker ps | tail -n1 | cut -f 1 -d \" \")\n",
    "echo \"Container ID: $container_id\"\n",
    "\n",
    "# copy modified scoring script again\n",
    "docker cp AD_score.py $container_id:/var/azureml-app/score.py\n",
    "sleep 1\n",
    "\n",
    "# stop the container\n",
    "docker restart $container_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the container\n",
    "\n",
    "**Note**, you probably have to run the above cell twice for the change to score.py to ahve an effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load telemetry data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "\n",
    "print(\"Reading data ... \", end=\"\")\n",
    "telemetry = pd.read_csv(os.path.join('data', 'telemetry.csv'))  # , parse_dates=['datetime'])\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Parsing datetime...\", end=\"\")\n",
    "telemetry['datetime'] = pd.to_datetime(telemetry['datetime'], format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "telemetry.columns = ['timestamp', 'machineID', 'volt', 'rotate', 'pressure', 'vibration']\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "def test_docker(telemetry, n=None):\n",
    "    \"\"\"\n",
    "        n is the number of sensor readings we are simulating\n",
    "        \"\"\"\n",
    "\n",
    "    if not n:\n",
    "        n = telemetry.shape[0]\n",
    "\n",
    "    machine_ids = [1] # telemetry['machineID'].unique()\n",
    "    timestamps = telemetry['timestamp'].unique()\n",
    "\n",
    "    out_df = pd.DataFrame()\n",
    "    for timestamp in timestamps[:10]:\n",
    "        np.random.shuffle(machine_ids)\n",
    "        for machine_id in machine_ids:\n",
    "            data = telemetry.loc[(telemetry['timestamp'] == timestamp) & (telemetry['machineID'] == machine_id), :]\n",
    "            json_data = data.to_json()\n",
    "            input_data = {\"data\": json_data}\n",
    "\n",
    "            r = requests.post('http://localhost:8080/score', json=input_data)\n",
    "\n",
    "            result = pd.read_json(json.loads(r.json()))\n",
    "            out_df = out_df.append(result, ignore_index=True)\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processing ... \")\n",
    "out_df = test_docker(telemetry)\n",
    "print(\"Done.\")\n",
    "out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = telemetry['timestamp'].unique()\n",
    "timestamp = timestamps[0]\n",
    "machine_id = 1\n",
    "data = telemetry.loc[(telemetry['timestamp'] == timestamp) & (telemetry['machineID'] == machine_id), :]\n",
    "json_data = json.dumps(data.to_json())\n",
    "json_data\n",
    "new_data = pd.read_json(json.loads(json_data))\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 2  # set the number of values going into the running avg\n",
    "values = np.random.normal(0,1,100)\n",
    "values = np.cumsum(values)\n",
    "\n",
    "\n",
    "running_avgs = []\n",
    "\n",
    "for value in values:\n",
    "    raw_data = {\"value\": value, \"n\": n}\n",
    "\n",
    "    r = requests.post('http://localhost:8080/score', json=raw_data)\n",
    "\n",
    "    result = json.loads(r.json())\n",
    "    running_avgs.append(result)\n",
    "\n",
    "plt.plot(values)\n",
    "plt.plot(running_avgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push the updated container to ACR\n",
    "\n",
    "**First**, test your Docker container again (run the json query above), to ensure that the changes are having the expected effect. \n",
    "\n",
    "**Then** you can push the image into ACR, so that it can be retrieved by the Azure ML SDK when you want to deploy your Webservice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$imageLocation\"\n",
    "\n",
    "image_location=$1\n",
    "\n",
    "# extract container ID\n",
    "container_id=$(docker ps | tail -n1 | cut -f 1 -d \" \")\n",
    "echo \"Container ID: $container_id\"\n",
    "\n",
    "# commit changes made in the container to the local copy of the image\n",
    "docker commit $container_id $image_location\n",
    "\n",
    "docker push $image_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to deploy the container to ACI, just to make sure everything behaves as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.image import ContainerImage\n",
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "# create configuration for ACI\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1, \n",
    "                                               tags={\"data\": \"some data\",  \"method\" : \"machine learning\"}, \n",
    "                                               description=\"Does machine learning on some data\")\n",
    "# pull the image\n",
    "image = ContainerImage(ws, name='my-docker-image', tags='1')\n",
    "\n",
    "# deploy webservice\n",
    "service_name = 'my-web-service'\n",
    "service = Webservice.deploy_from_image(deployment_config = aciconfig,\n",
    "                                            image = image,\n",
    "                                            name = service_name,\n",
    "                                            workspace = ws)\n",
    "service.wait_for_deployment(show_output = True)\n",
    "print(service.state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10  # set the number of values going into the running avg\n",
    "values = np.random.normal(0,1,100)\n",
    "values = np.cumsum(values)\n",
    "\n",
    "\n",
    "running_avgs = []\n",
    "\n",
    "for value in values:\n",
    "    raw_data = json.dumps({\"value\": value, \"n\": n})\n",
    "    raw_data = bytes(raw_data, encoding = 'utf8')\n",
    "    \n",
    "    # predict using the deployed model\n",
    "    result = json.loads(service.run(input_data=raw_data))\n",
    "\n",
    "    running_avgs.append(result)\n",
    "\n",
    "plt.plot(values)\n",
    "plt.plot(running_avgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up resources\n",
    "\n",
    "To keep the resource group and workspace for other tutorials and exploration, you can delete only the ACI deployment using this API call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy to AKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the AKS Cluster\n",
    "\n",
    "The following code snippet demonstrates how to create the AKS cluster. This process takes around 20 minutes to complete. \n",
    "\n",
    "**Note:** Creating the AKS cluster is a one time process for your workspace. Once created, you can reuse this cluster for multiple deployments. If you delete the cluster or the resource group that contains it, then you must create a new cluster the next time you need to deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AksCompute, ComputeTarget\n",
    "\n",
    "# Use the default configuration (you can also provide parameters to customize this)\n",
    "prov_config = AksCompute.provisioning_configuration()\n",
    "\n",
    "aks_name = 'wopauli-aks-1' \n",
    "# Create the cluster\n",
    "aks_target = ComputeTarget.create(workspace = ws, \n",
    "                                    name = aks_name, \n",
    "                                    provisioning_configuration = prov_config)\n",
    "\n",
    "# Wait for the create process to complete\n",
    "aks_target.wait_for_completion(show_output = True)\n",
    "print(aks_target.provisioning_state)\n",
    "print(aks_target.provisioning_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach existing AKS cluster (optional)\n",
    "\n",
    "If you have existing AKS cluster in your Azure subscription, you can use it to deploy your image. The following code snippet demonstrates how to attach a cluster to your workspace.\n",
    "\n",
    "**Important:** Only AKS version 1.11.2 is supported.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the resource id from https://porta..azure.com -> Find your resource group -> click on the Kubernetes service -> Properties\n",
    "# resource_id = '/subscriptions/<your subscription id>/resourcegroups/<your resource group>/providers/Microsoft.ContainerService/managedClusters/<your aks service name>'\n",
    "\n",
    "# # Set to the name of the cluster\n",
    "# aks_name = 'wopauli-aks-1'  \n",
    "\n",
    "# # Attatch the cluster to your workgroup\n",
    "# aks_target = AksCompute.attach(workspace=ws, name=aks_name, resource_id=resource_id)\n",
    "\n",
    "# # Wait for the operation to complete\n",
    "# aks_target.wait_for_completion(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy your web service\n",
    "\n",
    "The following code snippet demonstrates how to deploy the image to the AKS cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import Webservice, AksWebservice\n",
    "from azureml.core.image import ContainerImage\n",
    "\n",
    "# pull image from ACR\n",
    "image = ContainerImage(ws, name='my-docker-image', tags='8')\n",
    "\n",
    "# Set configuration and service name\n",
    "aks_config = AksWebservice.deploy_configuration()\n",
    "aks_service_name ='wopauli-aks-service'\n",
    "# Deploy from image\n",
    "aks_service = Webservice.deploy_from_image(workspace = ws, \n",
    "                                            name = aks_service_name,\n",
    "                                            image = image,\n",
    "                                            deployment_config = aks_config,\n",
    "                                            deployment_target = aks_target)\n",
    "# Wait for the deployment to complete\n",
    "aks_service.wait_for_deployment(show_output = True)\n",
    "print(aks_service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip:** If there are errors during deployment, use `aks_service.get_logs()` to view the AKS service logs. The logged information may indicate the cause of the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aks_service.serialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "msauthor": "sgilley"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
