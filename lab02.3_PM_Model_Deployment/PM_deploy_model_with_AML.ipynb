{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model deployment to ACI\n",
    "\n",
    "We finished the last Notebook by finding best fitting model using automated ML and registering it to our Azure ML account. In this Notebook, we deploy this model to an ACI instance and test it by scoring data against it. Scoring here happens in near-realtime, meaning that the data we score is pre-computed for us (such as a nightly batch job). Scoring can also happen in realtime, but as we will explore in a later Notebook, this requires more work. For predictive maintenance, realtime scoring is usually not needed, because models are used to predict when a machine is going to *about to* fail, which gives us some time to run unscheduled maintenance and replace parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the setup we have already created an AML workspace. Let's load the workspace and create an experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.train.automl.run import AutoMLRun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the workspace directly from the config file we created in the early part of the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = os.path.expanduser('~')\n",
    "config_path = os.path.join(home_dir, 'aml_config')\n",
    "ws = Workspace.from_config(path = config_path)\n",
    "\n",
    "experiment_name =  'pred-maint-automl' # choose a name for experiment\n",
    "project_folder = '.' # project folder\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "\n",
    "output = {}\n",
    "output['SDK version'] = azureml.core.VERSION\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace'] = ws.name\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "output['Project Directory'] = project_folder\n",
    "output['Experiment Name'] = experiment.name\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.DataFrame(data = output, index = ['']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "\n",
    "print(\"SDK Version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load the test data, not to evaluate the model but only to use it to get some predictions from our deployed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r X_test\n",
    "%store -r y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a scoring script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of the deployment consists of pointing to the model we want to deploy. We can simply provide the model name, which was given to us at the time we registered the model. We can also go to the Azure portal to look up the model name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a quick sanity check to ensure that the model exists and can be loaded (loading the model in the current session is not required for deployment).\n",
    "\n",
    "**Note**: You have to updated the `model_name` below. You can find the model name in your workspace in the azure portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "model_name = \"ENTER_REGISTERED_MODEL_NAME\"\n",
    "\n",
    "model = Model(workspace = ws, name = model_name)\n",
    "print(model.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a scoring script that will run every time we make a call to the deployed model. The scoring script consists of an `init` function that will load the model and a `run` function that will load the data we provide at score time and use the model to obtain predictions.\n",
    "\n",
    "**Note**: You have to updated `model_name` below by replacing the placeholder with the name of your registered model. You can find the model name in your workspace in the azure portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import pickle\n",
    "import json\n",
    "import numpy\n",
    "from sklearn.externals import joblib\n",
    "from azureml.core.model import Model\n",
    "import azureml.train.automl\n",
    "\n",
    "model_name = \"ENTER_REGISTERED_MODEL_NAME\"\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    model_path = Model.get_model_path(model_name = model_name) # this name is modeld.id of model that we want to deploy\n",
    "    # model_path = Model.get_model_path('model.pkl') # select this if deploying model from file\n",
    "    # deserialize the model file back into a sklearn model\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "def run(rawdata):\n",
    "    try:\n",
    "        data = json.loads(rawdata)['data']\n",
    "        data = numpy.array(data)\n",
    "        result = model.predict(data)\n",
    "    except Exception as e:\n",
    "        result = str(e)\n",
    "        return json.dumps({\"error\": result})\n",
    "    return json.dumps({\"result\":result.tolist()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a conda environment file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by retrieving the run ID for the automl experiment we ran in the last Notebook and pasting it in for the `run_id` argument in the `AutoMLRun` function below.\n",
    "\n",
    "**Note**: You have to updated `run_id` below. You can find the `run_id` under the experiments in the azure portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_run = AutoMLRun(experiment = experiment, run_id = 'ENTER_AUTOML_RUN_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a `yml` file for the conda environment that will be used to run the scoring script above. To ensure consistency of the scored results with the training results, the SDK dependencies need to mirror development environment (used for model training). In the code snippet below we need to provide `best_iter_num` with the number that corresponds to the best iteration from the AutoML Run ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_iter_num = 6 # change this to the desired iteration run\n",
    "dependencies = ml_run.get_run_sdk_dependencies(iteration = best_iter_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in ['azureml-train-automl', 'azureml-sdk', 'azureml-core']:\n",
    "    print('{}\\t{}'.format(p, dependencies[p]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can automatically generate a Conda dependencies YAML file. This can serve as a baseline, which we can then modify to create the Conda environment we want to use in production. Note that there may be a lot of packages that we required during training but aren't needed in production for scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "myenv = CondaDependencies.create(conda_packages=['scikit-learn'])\n",
    "# myenv.add_pip_package(\"azureml-train\")\n",
    "# myenv.add_pip_package(\"azureml-train-automl\")\n",
    "# myenv.add_pip_package(\"azureml-core\")\n",
    "\n",
    "with open(\"myenv.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat myenv.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make modifications to the above file to get the following Conda dependencies file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile myenv.yml\n",
    "name: myenv\n",
    "channels:\n",
    "  - defaults\n",
    "dependencies:\n",
    "  - pip:\n",
    "    - scikit-learn==0.19.1\n",
    "    - azureml-sdk[automl]==0.1.74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat myenv.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a docker image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the scoring script and conda environment file, we can now create a docker image that will host the scoring script and a Python executable that meets the conda requirement dependencies laid out in the YAML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.image import Image, ContainerImage\n",
    "\n",
    "image_config = ContainerImage.image_configuration(runtime = \"python\",\n",
    "                                 execution_script = \"score.py\",\n",
    "                                 conda_file = \"myenv.yml\",\n",
    "                                 tags = {'area': \"digits\", 'type': \"automl_classification\"},\n",
    "                                 description = \"Image for automl classification sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the image config file above we now create a Docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "image_name = experiment_name + \"-img\"\n",
    "\n",
    "image = Image.create(name = image_name,\n",
    "                     models = [model], \n",
    "                     image_config = image_config, \n",
    "                     workspace = ws)\n",
    "\n",
    "image.wait_for_creation(show_output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the image creation fails, this is how we can access the log file and examine what went wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image.image_build_log_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the image location that will be used when the imaged is pulled down from Docker hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image.image_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if the image was created in another session and we just wanted to point to it in this session, then we can just pass the image name and workspace to the `Image` function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image(name = experiment_name + \"-img\", workspace = ws, version=1)\n",
    "print(image.image_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Image as web service on ACI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to deploy our image as a web service on ACI. To do so, we first create a config file and then pass it to `deploy_from_image` along with a name for the service, the image we created in the last step, and our workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores = 1, \n",
    "                                               memory_gb = 1, \n",
    "                                               tags = {\"method\" : \"automl\"}, \n",
    "                                               description = 'Predictive maintenance using auto-ml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a service with the same name already exists, we can delete it by calling the `delete` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from azureml.core.webservice import Webservice\n",
    "\n",
    "aci_service_name = experiment_name + \"-aci\"\n",
    "print(aci_service_name)\n",
    "# aci_service.delete()\n",
    "aci_service = Webservice.deploy_from_image(deployment_config = aciconfig,\n",
    "                                           image = image,\n",
    "                                           name = aci_service_name,\n",
    "                                           workspace = ws)\n",
    "aci_service.wait_for_deployment(True)\n",
    "print(aci_service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we can get logs from the deployed service, which can help us debug any issues that cause the deployment to fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = aci_service.get_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "ll = re.findall(r\"\\{.*\\}\", logs)\n",
    "dd = [json.loads(l) for l in ll]\n",
    "\n",
    "for k in dd:\n",
    "    if 'level' in k.keys():\n",
    "        if k['level'] == 'INFO': \n",
    "            try:\n",
    "                j = json.loads(k['message'])\n",
    "                print(j['message'])\n",
    "            except:\n",
    "                print(k['message'])\n",
    "        if k['level'] == 'ERROR': \n",
    "            print('================================================================================')\n",
    "            print(json.loads(k['message'])['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative deployments (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two other ways that we could have launched our ACI deployment. The first one is by deploying directly from the image config file and the registered model. In this scenario, the deployment will first create the image from the registered model, and then deploy the docker container from the base image. So we combine two steps (image creation, service creation) into a single step. However, behind the scenes the steps still run individually and create corresponding resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aci_service.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from azureml.core.webservice import Webservice\n",
    "\n",
    "aci_service_name = experiment_name + \"-aci\"\n",
    "print(aci_service_name)\n",
    "\n",
    "# aci_service = Webservice.deploy_from_model(deployment_config = aciconfig,\n",
    "#                                        image_config = image_config,\n",
    "#                                        models = [model], # this is the registered model object\n",
    "#                                        name = aci_service_name,\n",
    "#                                        workspace = ws)\n",
    "# aci_service.wait_for_deployment(show_output = True)\n",
    "# print(aci_service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we launched the ACI docker container from the image config file and the registered model. But we can take one further step back and simply provide the model pickle file and let it register the model, then create a docker image from it and finally launch an ACI docker container from the image. In this case, we are combining three steps into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aci_service.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import Webservice\n",
    "\n",
    "aci_service_name = experiment_name + \"-aci\"\n",
    "print(aci_service_name)\n",
    "\n",
    "# aci_service = Webservice.deploy(deployment_config = aciconfig,\n",
    "#                                 image_config = image_config,\n",
    "#                                 model_paths = ['model.pkl'],\n",
    "#                                 name = aci_service_name,\n",
    "#                                 workspace = ws)\n",
    "\n",
    "# aci_service.wait_for_deployment(show_output = True)\n",
    "# print(aci_service.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aci_service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining many steps into one may save us a few lines of code, but it has the disadvantage of appearing to over-simplify the workflow. So it is probably best to avoid doing it, especially for production systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Web Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to test our web service. To begin with, we will point to our service using `Webservice`. Note that we've already done this in the last step, so in the current session this is not a necessary step, but since we want to be able to test the service from any Python session, we will point to the service again here. There is next to no overhead in doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.image import Image, ContainerImage\n",
    "from azureml.core.webservice import Webservice\n",
    "\n",
    "aci_service_name = experiment_name + \"-aci\"\n",
    "\n",
    "aci_service = Webservice(workspace = ws, name = aci_service_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now proceed to testing the service. To do so, we will take a few random samples from `X_test` and dump its content into a json string (with UTF-8 encoding). This will act as the data that we intend to score. We can pass this data to the service using the `run` method, and it will return the predictions to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "sample_indices = np.random.permutation(X_test.shape[0])[0:n]\n",
    "\n",
    "test_samples = json.dumps({\"data\": X_test.iloc[sample_indices, :].values.tolist()})\n",
    "test_samples = bytes(test_samples, encoding = 'utf8')\n",
    "print(test_samples)\n",
    "\n",
    "# predict using the deployed model\n",
    "prediction = aci_service.run(input_data = test_samples)\n",
    "print('**********************************************')\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing against a REST client (optional)\n",
    "\n",
    "We were able to successfully test the scoring script from Python by calling the `run` method on it. But what happened in the background? We executed a REST call to the web application. Let's now re-do this from a REST client to confirm that everything works.\n",
    "\n",
    "1. Install a REST client for your browser such as *RESTClient, a debugger for RESTful web services.* (Firefox)\n",
    "2. In the section called **Request Headers** set Name to **Content-Type** and Attribute Value to **application/json**.\n",
    "3. Copy the content of data snippet above to https://jsonformatter.org/ and validate it to make sure it's clean.\n",
    "4. Paste the clean json into the section of the REST client called **Body**, set method to **POST** and add the API address (what we get when we run `myservice.scoring_uri`) to the section called **URL**.\n",
    "5. Finally, hit **SEND** and scroll down to see your results in the tab called **Response**.\n",
    "6. Note how you can see the original POST call, which looks like `curl -X POST -H 'Content-Type: application/json' -i 'http://<SCORING_URI>:80/score' --data ...`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "In the above example, we took the data to be scored directly from `X_test`. But this data had already been pre-processed for us and was ready for scoring. A more realistic scenario involves getting raw data, pre-processing it and then feeding it to the deployed model for scoring. In this lab, we will implement this.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the telemetry data\n",
    "# compute moving average telemetries (you will need a way to compute moving averages at test time)\n",
    "# append maintenance history and failure history to it\n",
    "# finally, score the data to obtain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
