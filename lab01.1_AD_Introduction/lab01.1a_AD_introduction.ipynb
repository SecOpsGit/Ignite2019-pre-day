{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Copyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT License."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Introduction to anomaly detection\n\nThis notebook provides a short primer on detecting contextual anomalies in time series data. \n\nAfter completing this notebook, you will have developed an intuitive understanding of a common algorithmic approach to anomaly detection that we are going to be relying on throughout this course.\n\nWe will try to achieve this goal using the following steps:\n1. Create a hypothetical time series dataset\n2. Apply simple, but common anomaly detection algorithm\n3. Understand the limitations of this approach in dealing with\n    - seasonal trends (e.g. higher energy consumption on the weekends than on weekdays)\n    - linear trends (e.g. growth over time)\n4. Improve the anomaly detection algorithm to be robust against seasonal and linear trends\n\n> **Important:** We created these notebooks such that you can run them as a Jupyter, or as a databricks notebook.  We ask you to keep this in mind because this has several implications:\n- In several cells of code you will find a hint about enabling or disabling lines of codes, depending whether you are running thess notebooks on Jupyter or on Databricks."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We begin by importing all necessary Python modules, and by setting some global configuration settings."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#%matplotlib inline \nimport os\n\n#  Let's start with loading python modules we need for this course\nfrom rstl import STL\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn\nimport numpy as np\nimport urllib\n\n# you can change this to adjust the size of figures to work well with your screen resolution\nwide_figure = (16,4)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Create a hypothetical time series dataset\n\nWe begin by creating a timeseries of hypothetical telemetry data.  We will use this timeseries throughout this notebook. The advantage of using your own timeseries is that because we know its properties, we can make precise predictions about the outcome of our analyses. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "n_measures = 24 * 365 # we simulate one measurement per hour for one year\n\n# define nature of anomalies\np_anom = 0.001 # probability of anomalies\nloc, scale = 5, .1 # we sample anomalies from a normal distribution with mean 'loc' and std 'scale'\n\nx = np.linspace(1, n_measures, n_measures)\nx_tics = x / 24 # we plot x-axis in days\n\n# let's create a time series normal data, by drawing independent samples from a normal distribution\ny_reg = np.random.normal(size=int(round(n_measures * (1 - p_anom)))).tolist()\n\n# anomalies are also drawn from a normal distribution, but with a different mean and standard deviation\ny_anom = np.random.normal(loc=loc, scale=scale, size=int(round(n_measures * (p_anom)))).tolist()\n\n# we concatentate the two lists into an array, and shuffle the array\ny = np.array(y_reg + y_anom)\nnp.random.shuffle(y)\n\n# let's see what this looks like\nplt.close()\nfig = plt.figure(figsize=wide_figure)\nplt.plot(x_tics,y)\nplt.xlabel(\"Days\")\nplt.ylabel(\"Telemetry (Measurement)\")\ndisplay()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Basic approaches to anomaly detection\n\nLet's start with one of the simplest approaches to anomaly detection:\n1. We calculate the mean and the standard deviation of all instances (measurements)\n2. We mark every instance that is too far from the mean as an anomaly"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tolerance = 4 # how many standard deviations does an instance have to be from the mean in order to be an anomaly\n\nm = np.mean(y)\nstd = np.std(y)\ncutoff = m + tolerance * std\n\n# find the array indices of extreme values (anomalies)\nidx = np.where(y > cutoff)[0].tolist()\n\n# create an array that is all NaN, except for the anomalies\nanoms = np.full(y.shape[0], np.nan)\nanoms[idx] = y[idx] # copy the value of the anomaly\nanoms_orig = np.array(anoms) # let's store a copy of the anomalies for later\n\nplt.close()\nfig = plt.figure(figsize=wide_figure)\nplt.plot(x_tics, y)\nplt.plot(x_tics, [cutoff] * len(x_tics), '--')\nplt.plot(x_tics, anoms, 'o')\nplt.xlabel(\"Days\")\nplt.ylabel(\"Telemetry (Measurement)\")\ndisplay()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Hands-on lab\n\nCan you write a function that does what we did above: accept a time series as input, detects the anomalies, and returns the anomalies and the cutoff.  The function should also accept a `keyword argument` to specify the above `tolerance` of the anomaly detection.  \n\nThis exercise will ensure you understand how we defined anomalies and offer you a chance to practice your Python skills."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\n\ndef detect_anomalies():\n\n    # your code goes here\n    \n    return anoms, mean, cutoff",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "If you can't get the solution. Run the uncomment the following cell and execute twice."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# %load ../solutions/detect_anomalies.py",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now test your code. Are the results the same as before?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "anoms, m, cutoff = detect_anomalies(y)\n\nplt.close()\nfig = plt.figure(figsize=wide_figure)\nplt.plot(x_tics, y)\nplt.plot(x_tics, anoms, 'o')\nplt.plot(x_tics, [cutoff] * len(x_tics), '--')\nplt.plot(x_tics, anoms, 'o')\nplt.xlabel(\"Days\")\nplt.ylabel(\"Telemetry (Measurement)\")\ndisplay()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### End of Lab"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Limitations of this approach\n\nLooks like this worked really well, but what if you use some more realistic data. Real-world data often has seasonal or linear trends.  Cars tend to consume more oil per mile when they get older.  Power consumption is typically higher on weekdays than on weekends, when office buildings are empty.\n\n### Seasonal trends\n\nLet's see how this approach performs if the data have some seasonality: weekly cycles."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "sample_size = 24 * 7 * 4 # for display purposes\n\n# seasonal trend (weekly)\namp = 1.5 # amplitude of the weekly effect\nmod = amp * np.cos(2 * np.pi * x / 24 / 7)\ny_mod = y + mod # create a new timeseries by adding the seasonal effect to the original data\n\nplt.close()\nfig, ax = plt.subplots(1,3, figsize=(14,8))\nax[0].plot(x_tics[:sample_size], y[:sample_size])\nax[1].plot(x_tics[:sample_size], mod[:sample_size])\nax[2].plot(x_tics[:sample_size], y_mod[:sample_size])\nax[0].set_xlabel('Time (days)')\nax[0].set_ylabel(\"Telemetry (Measurement)\")\nax[1].set_title('weekly cycle')\nax[2].set_title('original data + weekly cycle')\nax[0].set_title('original data')\ndisplay()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's use the same approach as before and see how it performs on this new time series."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "anoms, m, cutoff = detect_anomalies(y_mod)\n\nplt.close()\nfig, ax = plt.subplots(1,2, figsize=(12,8))\n\nax[0].plot(x_tics, y_mod)\nax[0].plot(x_tics, anoms, 'o')\nax[0].plot(x_tics, [cutoff] * len(x_tics), '--')\nax[0].set_xlabel(\"Days\")\nax[0].set_ylabel(\"Telemetry (Measurement)\")\nax[0].set_title(\"original data + weekly cycle\")\n\nax[1].plot(x_tics, y)\nax[1].plot(x_tics, anoms_orig, 'o')\nax[1].set_xlabel('Time (days)')\nax[1].set_title(\"original\")\ndisplay()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "It looks like our approach was still able to recover some of the anomalies, but this would clearly not be acceptable.  Imagine you have a situation where not detecting an anomaly could lead to massive costs due to equipment damage!\n\n### Linear trends\n\nLet's add a linear trend to our data to see how the algorithm performs."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "slope = .001 # slope of the linear trend\nlinear_trend = x * slope\ny_mod_lin = y_mod + linear_trend\n\nplt.close()\nfig, ax = plt.subplots(1,3, figsize=(14,8), sharey=True)\nax[0].plot(x_tics[:sample_size], y_mod[:sample_size])\nax[1].plot(x_tics[:sample_size], linear_trend[:sample_size])\nax[2].plot(x_tics[:sample_size], y_mod_lin[:sample_size])\nax[0].set_xlabel('Time (days)')\nax[0].set_title('original + weekly cycle')\nax[1].set_title('linear trend')\nax[2].set_title('original + weekly cycle + linear trend')\ndisplay()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Run the algorithm and see how it does. If you don't see any anomalies, try lowering the `tolerance`."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tolerance = 4\n\nanoms, m, cutoff = detect_anomalies(y_mod_lin, tolerance=tolerance)\n\nplt.close()\nfig, ax = plt.subplots(1,2, figsize=(12,8))\n\nax[0].plot(x_tics, y_mod_lin)\nax[0].plot(x_tics, anoms, 'o')\nax[0].plot(x_tics, [cutoff] * len(x_tics), '--')\nax[0].set_xlabel('Time (days)')\nax[0].set_title('orig + weekly cycle + linear trend')\n\nax[1].plot(x_tics, y)\nax[1].plot(x_tics, anoms_orig, 'o')\nax[1].set_xlabel('Time (days)')\nax[1].set_title(\"original\")\ndisplay()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "What do you think? Did it work? You could try lowering the `tolerance` (e.g. 2). Did that help?\n\nIt looks like if we have seasonal and linear trends in our time series, the simple approach from above doesn't work at all!\n\nLet's see whether we can fix that.\n\n## Improve the anomaly detection algorithm to be robust against seasonal and linear trends\n\nOne common approach to this dilema is STL, a versatile and robust method for decomposing time series. STL is an acronym for \"Seasonal and Trend decomposition using Loess\". \n\nThe STL method was developed by Cleveland, Cleveland, McRae, & Terpenning (1990).\n\nSTL has several strengths:\n- It handles *any type* of seasonality, including monthly, weekly, daily seasonality.\n- The seasonal component is allowed to change over time, and the rate of change can be controlled by the user.\n- The smoothness of the trend-cycle can also be controlled by the user.\n- It can be robust to outliers (i.e., the user can specify a robust decomposition), so that occasional unusual observations will not affect the estimates of the trend-cycle and seasonal components.\n\nLet's give it a try.\n\nThe STL decomposition method accepts a time series as input, and decomposes the time series into a linear trend, seasonal trend, and a remainder.  Put another way, the remainder is what is left if we subtract the linear and seasonal trends from the original time series."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "stl = STL(y_mod_lin, freq=24*7, s_window='periodic')\n\nplt.close()\nfig, ax = plt.subplots(1,4, figsize=(16,8))\nax[0].plot(x, y_mod_lin) # the time series\nax[0].set_title(\"original + linear + seasonal trend\")\nax[1].plot(x, stl.trend) # linear trend\nax[1].set_title(\"linear trend\")\nax[2].plot(x, stl.seasonal) # sesonal trend\nax[2].set_title(\"seasonal trend\")\nax[3].plot(x, stl.remainder) # remainder\nax[3].set_title(\"remainder\")\ndisplay()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now let's try whether we can apply our approach to anomaly detection. \n\nIf STL did its job, the `remainder` should look just like the original time series, before we added linear and seasonal trends. So if we run our original approach to anomaly detection, we should be able to detect the same anomalies on the remainder after running STL decomposition as we did on the original data."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "anoms, m, cutoff = detect_anomalies(stl.remainder)\n\nplt.close()\nfig, ax = plt.subplots(1,2, figsize=(12,8))\nax[0].plot(x_tics, stl.remainder)\nax[0].plot(x_tics, anoms, 'o')\nax[0].plot(x_tics, [cutoff] * len(x_tics), '--')\nax[0].set_xlabel('Time (days)')\nax[0].set_title(\"stl.remainder\")\n\nax[1].plot(x_tics, y)\nax[1].plot(x_tics, anoms_orig, 'o')\nax[1].set_xlabel('Time (days)')\nax[1].set_title(\"original data\")\ndisplay()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "It looks like we were able to recover all the anomalies!\n\n## Conclusion\n\nBy now you should have an intuitive understanding of the approach to anomaly detection we will follow in this lab. \n\nWe will follow a divide and conquer strategy, of first removing linear and seasonal trends, so that we can simply mark all extreme values as anomalies.\n\nIn the process of this notebook you were also introduced to the following:\n- How to generate a dataset for exploring your idea.  Using an idealized dataset can be very helpful, because you know exactly what the result should look like.  For example, in our case we knew that we were successful if could discover the same anomalies as the ones that we had discovered orignally.\n- A simple, but common approach to detecting point anomalies.\n- The STL algorithm, a very powerful tool that can be used even outside of anomaly detection, as part of data preparation or preprocessing."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Hands-on lab\n\nCan you update your function for anomaly detection from above to perform STL first, before applying anomaly detection?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nfrom rstl import STL\n\ndef detect_anomalies_stl(ts, tolerance=4):\n    \n    ## your code goes here\n    \n    return anoms, stl.remainder",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# %load ../solutions/detect_anomalies_stl.py",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's see whether the results are as expected.  If things went well, you should see to figures that almost look identical."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "anoms, stl_remainder = detect_anomalies_stl(y_mod_lin, tolerance=4)\n\nplt.close()\nfig, ax = plt.subplots(1,2, figsize=(12,8))\nax[0].plot(x_tics, stl_remainder)\nax[0].plot(x_tics, anoms, 'o')\nax[0].set_xlabel('Time (days)')\nax[0].set_title(\"stl.remainder\")\n\nax[1].plot(x_tics, y)\nax[1].plot(x_tics, anoms_orig, 'o')\nax[1].set_xlabel('Time (days)')\nax[1].set_title(\"original data\")\ndisplay()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### End of lab"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## The end"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Copyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT License."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "name": "01_AD_introduction",
    "notebookId": 4063271094430089
  },
  "nbformat": 4,
  "nbformat_minor": 1
}