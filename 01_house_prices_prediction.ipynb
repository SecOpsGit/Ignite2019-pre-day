{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## House Prices Prediction\n",
    "\n",
    "In this tutorial we will prepare a dataset with houses characteristics and selling prices and train a regression model for sales price prediction.\n",
    "\n",
    "The dataset to be used is the [Ames Housing Dataset](https://www.openintro.org/stat/data/?data=ames), which has variables describing (almost) every aspect of residential homes in Ames, Iowa.\n",
    "\n",
    "A detailed description of the variables in this dataset can be found [here](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt).\n",
    "\n",
    "We begin by importing the necessary packages and setting some notebook options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import make_scorer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we download the dataset described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"./data\"\n",
    "os.makedirs(data_folder, exist_ok = True)\n",
    "\n",
    "!wget https://www.openintro.org/stat/data/ames.csv -O ./data/ames.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load the dataset into a Pandas data frame, visualize the first 10 rows, and print the total number of rows and columns. We notice that this dataset has 2930 rows and 82 columns. Our response variable is the column named `SalePrice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing = pd.read_csv(\"./data/ames.csv\")\n",
    "\n",
    "df_housing.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We describe all columns and notice several things:\n",
    "  - the majority of the variables are categorical\n",
    "  - some categorical variables are wrongly encoded as numeric\n",
    "  - some numeric variables are wrongly encoded as categorical\n",
    "  - there are several missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_housing.describe(include = \"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the `Order` and `PID` columns because they are unique identifiers and won't help predicting the house price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing[\"Order\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing[\"PID\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing.drop(\"Order\", axis = 1, inplace = True)\n",
    "df_housing.drop(\"PID\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by treating the missing values. To better analyze this,we create a function that builds a table with the missing percentage for each variable that has missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_missing_ratio(df):\n",
    "    df_na = (df.isnull().sum() / len(df)) * 100\n",
    "    df_na = df_na.drop(df_na[df_na == 0].index).sort_values(ascending = False)\n",
    "    display(pd.DataFrame({'Missing Ratio' :df_na}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_missing_ratio(df_housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here we apply some strategies for inputting missing values, according to some hints we can get from the dataset description. \n",
    "\n",
    "For example, for some categorical variables a missing value represents a category like \"None\", and for some numerical variables it represents the value 0.\n",
    "\n",
    "For variables with relatively few missing values we can perform basic inputations like the median value for numeric variables and the mode value for categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_none = [\"Pool.QC\", \"Misc.Feature\", \"Alley\", \"Fence\", \"Fireplace.Qu\", \n",
    "             \"Garage.Type\", \"Garage.Finish\", \"Garage.Qual\", \"Garage.Cond\",\n",
    "            \"Bsmt.Exposure\", \"Bsmt.Cond\", \"Bsmt.Qual\", \"Mas.Vnr.Type\"]\n",
    "for var in fill_none:\n",
    "    df_housing[var] = df_housing[var].fillna(\"None\")\n",
    "    \n",
    "fill_zero = [\"Garage.Yr.Blt\", \"BsmtFin.Type.2\", \"BsmtFin.Type.1\", \"Bsmt.Half.Bath\", \n",
    "             \"Bsmt.Full.Bath\", \"Total.Bsmt.SF\", \"Bsmt.Unf.SF\", \"BsmtFin.SF.1\", \n",
    "             \"BsmtFin.SF.2\", \"Garage.Area\", \"Garage.Cars\", \"Mas.Vnr.Area\"]\n",
    "for var in fill_zero:\n",
    "    df_housing[var] = df_housing[var].fillna(0)\n",
    "\n",
    "df_housing[\"Lot.Frontage\"] = df_housing.groupby(\"Neighborhood\")[\"Lot.Frontage\"].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "df_housing['Electrical'] = df_housing['Electrical'].fillna(df_housing['Electrical'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_missing_ratio(df_housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing.loc[df_housing[\"Lot.Frontage\"].isnull(), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have inputed missing values for `Lot.Frontage` with the median values of `Lot.Frontage` by `Neighborhood`, but there are still missing values for that variable.\n",
    "\n",
    "This is because there is one neighborhood with only one house and its `Lot.Frontage` value is missing. And there is another neighborhood with only two houses with both values for `Lot.Frontage` also missing. \n",
    "\n",
    "We will then discard those records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing = df_housing.dropna()\n",
    "\n",
    "compute_missing_ratio(df_housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we correct some data types, according to our interpretation of continuous and categorical variables in this dataset. We represent numerical continuous values as float numbers and categorical as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "response_var = [\"SalePrice\"]\n",
    "\n",
    "numeric_vars = [\"Lot.Frontage\", \"Lot.Area\", \"Mas.Vnr.Area\", \"BsmtFin.SF.1\", \"BsmtFin.SF.2\", \n",
    "                \"Bsmt.Unf.SF\", \"Total.Bsmt.SF\", \"X1st.Flr.SF\", \"X2nd.Flr.SF\", \"Low.Qual.Fin.SF\", \n",
    "                \"Gr.Liv.Area\", \"Garage.Area\", \"Wood.Deck.SF\", \"Open.Porch.SF\", \"Enclosed.Porch\", \n",
    "                \"X3Ssn.Porch\", \"Screen.Porch\", \"Pool.Area\", \"Misc.Val\"]\n",
    "\n",
    "categorical_vars = [v for v in df_housing.columns if v not in numeric_vars + response_var]\n",
    "\n",
    "df_housing[response_var] = df_housing[response_var].astype(float)\n",
    "df_housing[numeric_vars] = df_housing[numeric_vars].astype(float)\n",
    "df_housing[categorical_vars] = df_housing[categorical_vars].astype(str)\n",
    "\n",
    "display(pd.DataFrame({\"Data Type\": df_housing.dtypes}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finishing the data cleaning, we then visualize relashionships between variables.\n",
    "\n",
    "We begin with scatterplots between `SalePrice` and continuous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df_housing, y_vars=response_var, x_vars=numeric_vars[0:7])\n",
    "sns.pairplot(df_housing, y_vars=response_var, x_vars=numeric_vars[7:13])\n",
    "sns.pairplot(df_housing, y_vars=response_var, x_vars=numeric_vars[13:19])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create boxplots of `SalesPrice` according to the categories given by the categorical variables.\n",
    "\n",
    "To better visualize this, we first encode each categorical variable by ordering its distinct category values according to the mean of `SalePrice` calculated for each category value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(frame, feature):\n",
    "    ordering = pd.DataFrame()\n",
    "    ordering['val'] = frame[feature].unique()\n",
    "    ordering.index = ordering.val\n",
    "    ordering['spmean'] = frame[[feature, 'SalePrice']].groupby(feature).mean()['SalePrice']\n",
    "    ordering = ordering.sort_values('spmean')\n",
    "    ordering['ordering'] = range(1, ordering.shape[0] + 1)\n",
    "    ordering = ordering['ordering'].to_dict()\n",
    "    \n",
    "    for cat, o in ordering.items():\n",
    "        frame.loc[frame[feature] == cat, feature + '_E'] = o\n",
    "    \n",
    "categorical_vars_E = []\n",
    "for q in categorical_vars:  \n",
    "    encode(df_housing, q)\n",
    "    categorical_vars_E.append(q + '_E')\n",
    "\n",
    "def boxplot(x, y, **kwargs):\n",
    "    sns.boxplot(x = x, y = y)\n",
    "    x = plt.xticks(rotation = 90)\n",
    "\n",
    "data = pd.melt(df_housing, id_vars = [\"SalePrice\"], value_vars = categorical_vars_E)\n",
    "g = sns.FacetGrid(data, col = \"variable\",  col_wrap = 5, sharex = False, sharey = False)\n",
    "g = g.map(boxplot, \"value\", \"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we plot the Spearman Correlation between `SalePrice` and each variable.\n",
    "\n",
    "For this to make sense for the categorical variables, we use the previous numeric ordered encoded values to represent each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def spearman(frame, features):\n",
    "    spr = pd.DataFrame()\n",
    "    spr['feature'] = features\n",
    "    spr['spearman'] = [frame[f].corr(frame['SalePrice'], 'spearman') for f in features]\n",
    "    spr = spr.sort_values('spearman')\n",
    "    plt.figure(figsize = (6, 0.25*len(features)))\n",
    "    sns.barplot(data = spr, y = 'feature', x = 'spearman', orient = 'h')\n",
    "    \n",
    "features = numeric_vars + categorical_vars_E\n",
    "spearman(df_housing, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we visualize the distribution of `SalePrice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_housing[response_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df_housing[response_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not going to perform any outlier analysis, feature selection or transformation here. Instead, we will try to model `SalePrice` directly using a non-linear regression algorithm.\n",
    "\n",
    "As an example, we will use Gradient Boosting Regression.\n",
    "\n",
    "The first step here is to split the dataset in a training portion and a test portion for final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_housing[numeric_vars + categorical_vars_E], \n",
    "                                                    df_housing[response_var],\n",
    "                                                    test_size = 0.4, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a loop for model training with grid search for hyperparameter selection and using the training data for 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = [{'n_estimators': [250,500,1000], 'max_depth': [4,8], 'min_samples_split': [2,4],\n",
    "                   'learning_rate': [0.01], 'loss': ['ls']}]\n",
    "\n",
    "scores = {'R2': make_scorer(r2_score), 'MAE': make_scorer(mean_absolute_error)}\n",
    "\n",
    "cv_models = {}\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(GradientBoostingRegressor(), parameter_grid, cv = 10, scoring = scores[score], n_jobs = -1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    cv_models[score] = clf\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the best model and compute metrics for the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = cv_models['R2'].best_estimator_\n",
    "\n",
    "mae = mean_absolute_error(y_test, best_model.predict(X_test))\n",
    "print(\"MAE on test dataset: %.4f\" % mae)\n",
    "\n",
    "y_test_predicted = best_model.predict(X_test)\n",
    "r2 = r2_score(y_test, y_test_predicted)\n",
    "print(\"R2 on test dataset: %.4f\" % r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute relative feature importances using the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importance = best_model.feature_importances_\n",
    "# make importances relative to max importance\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "var_names = np.asarray(numeric_vars + categorical_vars)\n",
    "fig = plt.figure(figsize=(12, 20))\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, var_names[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scores = {'R2': make_scorer(r2_score), 'MAE': make_scorer(mean_absolute_error)}\n",
    "\n",
    "clf = GradientBoostingRegressor(n_estimators = 500, max_depth = 4, min_samples_split = 2, \n",
    "                                learning_rate = 0.01, loss = \"ls\")\n",
    "scores = cross_validate(clf, X_train, y_train, scoring = scores, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_fitted = clf.fit(X_train, y_train)\n",
    "y_test_predicted = clf_fitted.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_test_predicted)\n",
    "print(\"MAE on test dataset: %.4f\" % mae)\n",
    "\n",
    "r2 = r2_score(y_test, y_test_predicted)\n",
    "print(\"R2 on test dataset: %.4f\" % r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[\"test_R2\"].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
