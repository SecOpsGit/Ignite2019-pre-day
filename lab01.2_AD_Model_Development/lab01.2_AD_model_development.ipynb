{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Copyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT License."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Model Development\n\nIn the introductory lab, we encountered alternative approaches to anomaly detection.  We implemented our own solution to labeling data points that were several standard deviations from the mean of all data points.  Then we estimated linear and seasonal trends in the time series, and removed them to perform anomaly detection on the remaining time series.\n\nHere, we are going to apply this approach to our telemetry data.  We will first develop a solution that processes all the data in **batch**, and then optimize the solution so that it detects anomalies **online**, whenever new sensor measurements arrive.\n\nThere are two reasons why we want to move to an online solution:\n- We don't want to repeatedely process historic data over and over again, this would be a waste of computing resources and time.\n- We should only store as much historic data as necessary for robust online anomaly detection.\n\nIn this lab, we will do the following:\n- Use python port [pyculiarity](https://github.com/nicolasmiller/pyculiarity) of [Twitter's anomaly detection R library](https://github.com/twitter/AnomalyDetection)\n- Do some data preprocessing to clean up our timeseries data\n- Apply this library to detect anomalies in our telemetry data\n- Adapt our solution to perform online anomaly detection\n\nWe will have two objectives:\n1. Find a way to calculate running averages online\n2. Determine how much historic data we need to retain without losing our ability to robustly detect anomalies"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# %matplotlib inline\n\nimport pandas as pd\nimport numpy as np\nimport os\nfrom pyculiarity import detect_ts # python port of Twitter AD lib\nimport matplotlib.pyplot as plt # for plotting\nimport seaborn # for decent default settings for figures\nimport time # so we can time our operations\nimport inspect\nimport urllib",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Load data\n\nAs usual, we start by loading the data"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "base_path = 'https://sethmottstore.blob.core.windows.net'\ndata_dir = os.path.join(base_path, 'predmaint')\n\nprint(\"Reading data ... \", end=\"\")\ntelemetry = pd.read_csv(os.path.join(data_dir, 'telemetry.csv'))\nprint(\"Done.\")\n\nprint(\"Parsing datetime...\", end=\"\")\ntelemetry['datetime'] = pd.to_datetime(telemetry['datetime'], format=\"%m/%d/%Y %I:%M:%S %p\")\nprint(\"Done.\")\n\nprint(\"Reading reference anomaly detection results ... \", end=\"\")\nanoms_batch = pd.read_csv(os.path.join(data_dir, 'anoms.csv'))\nanoms_batch['datetime'] = pd.to_datetime(anoms_batch['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\nprint(\"Done.\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Data Preparation\n\nLet's pick a random sensor (e.g. volt) and machine id and see what we get."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "sensor = 'volt'\nmachine_id = 67\n\n# Select the slice of the telemetry for this machine \ndf_s = telemetry.loc[telemetry.loc[:, 'machineID'] == machine_id, ('datetime', sensor)]\ndf_s.columns = ['timestamp', 'value']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# let's take a peak at the data\nplt.close()\nfig = plt.figure(figsize=(16,4))\nplt.plot(df_s['timestamp'], df_s['value'])\ndf_s.describe()\ndisplay()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's try to run anomaly detection. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "results = detect_ts(df_s) # we use the detect_ts function of pyculiarity\nanoms = results['anoms']\n\nplt.close()\nplt.plot(df_s['timestamp'], df_s['value'])\nplt.plot(anoms['timestamp'], anoms['anoms'], 'o')\ndisplay()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Hmmm. Looks like we don't really get too many anomalies this way.  We could try to lower our thresholds. Should we try a lower threshold?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "alpha = 0.2 # The level of statistical significance with which to accept or reject anomalies. (default: 0.05)\n\nresults = detect_ts(df_s, alpha=alpha)\nanoms = results['anoms']\n\nplt.close()\nplt.plot(df_s['timestamp'], df_s['value'])\nplt.plot(anoms['timestamp'], anoms['anoms'], 'o')\ndisplay()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "That doesn't really seem too change to much. We could lower the threshold further, but maybe we need to take a completely different approach.\n\n## Denoising data\n\nThe data look really noisy. Maybe what we need to do is to calculate running averages, because it is not individual measurements that are a problem, but a *collection* of measurements.\n\nLet's start by aggregating sensor measurements over the last 12h.  Let's use exponential decay, so that more recent measurements inside the time window have more weight that those that are at the beginning of the time window."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "com = 6 # Specify decay in terms of center of mass (this approximates averageing over about twice as many hours)\nrm = df_s['value'].ewm(com=com).mean()\n\nplt.close()\nplt.plot(df_s['timestamp'], df_s['value'])\nplt.plot(df_s['timestamp'], rm)\ndisplay()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Interesting, now there seem to be some more apparent anomalies.  Apparently, sometimes sensor readings are off for several hours in a row.  Let's try to run anomaly detection on the smoother time series."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# we create a data frame with two columns (timestamp, value), which is what pyculiarity expects as input\ndf_smooth = pd.DataFrame(data={'timestamp': df_s['timestamp'], 'value': rm})\n\nresults = detect_ts(df_smooth)\nanoms = results['anoms']\n\n# we store the location of the last anomaly for later\nlast_anomaly = int(np.where(df_smooth['timestamp'] == results['anoms'].tail(1)['timestamp'].values[0])[0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt.close()\nplt.plot(df_smooth['timestamp'], df_smooth['value'])\nplt.plot(anoms['timestamp'], anoms['anoms'], 'o', alpha=.2)\nplt.xlabel(\"Dates\")\nplt.ylabel(\"Sensor Value\")\ndisplay()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Hands-on lab\n\nThat appears to work, but we want our scoring service to work in real-time.  That means we shouldn't be calculating running averages over all the historic data whenever we receive just one new row of data.  Instead we want to use the last existing value of the running average, integrate it with the new data, to get the new running average.  Put another way, we want to calculate running averages *online*, rather than in *batch*."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Calculating running averages online\n\nThe first step is to create a lightweight function that can calculate running avgs. \n\n[Welford's online](https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford's_Online_algorithm) algorithm for calculating running variance includes a method to also calculate the mean.  This method is also described in Donald Knuth's Art of Computer Programming, Vol 2, page 232, 3rd edition."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\n\ndef running_avg(ts, com=6):\n    rm_o = np.zeros_like(ts)\n    rm_o[0] = ts[0]\n    \n    for r in range(1, len(ts)):\n        curr_com = float(min(com, r))\n        rm_o[r] = <your solution goes here>\n    \n    return rm_o",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# %load ../solutions/running_avg.py",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's confirm that your solution is correct. If your online version produces the same results as the batch version, you should just see an orange line in the below plot.  That is, a perfect blend of red and yellow. If you also see red and yellow, that means your online solution falls short of implmenting the batch version for calculating running averages."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "rm_o = running_avg(df_s['value'].values)\n\nplt.close()\nplt.plot(df_smooth['timestamp'], df_smooth['value'], color='yellow')\nplt.plot(df_smooth['timestamp'], rm_o, alpha=.5, color='red')\ndisplay()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can also plot he difference between the two resulting time series.  Except for the couple of first values in the timeseries, this should all be zeros."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt.close()\nplt.plot(df_smooth['timestamp'], df_smooth['value'] - rm_o)\ndisplay()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### End of Lab\n\n## Optimizing the window over which to perform Anomaly Detection\n\nOne remaining optimization is to only use the last couple of values of the running average for anomaly detection, instead of running it repeatedely on the entire timeseries of the running average.  There is no general answer for how many recent values have to be included to get robust anomaly detection, because the choice of the size of this sliding window depends on all sorts of factors (e.g. machine, sensor, size of anomaly).  \n\nThis means we have to develop an empirical approach to finding the right size for this sliding window.   \n\nThe first step is to define a function that performs anomaly detection using only those values that fall within a specified time window leading up to the current value.\n\nThe smaller the window, the faster the anomaly detection algorithm runs.  However, if we make the window too small, the algorithm will no longer be able to detect linear and seasonal trends, and will fail.\n\nBelow is a function that allows you to test different window sizes for anomaly detection.  It expects a data frame with two columns (timestamp, value), a specification of the window size, and the row index of an anomaly in the provided data frame.  The function returns whether it found an anomaly and how long it took to run (in seconds)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def detect_ts_online(df_smooth, window_size, stop):\n    is_anomaly = False\n    run_time = 9999\n    \n    start_index = max(0, stop - window_size)\n    df_win = df_smooth.iloc[start_index:stop, :]\n    \n    start_time = time.time()\n    \n    # call pyculiarity's anomaly detection function\n    results = detect_ts(df_win, alpha=0.05, max_anoms=0.02, only_last=None, longterm=False, e_value=False, direction='both')\n    \n    run_time = time.time() - start_time\n    \n    if results['anoms'].shape[0] > 0:\n        timestamp = df_win['timestamp'].tail(1).values[0]\n        if timestamp == results['anoms'].tail(1)['timestamp'].values[0]:\n            is_anomaly = True\n            \n    return is_anomaly, run_time",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's run this function to test whether we can recover the last anomaly we previously discovered in this time series using the batch method.  We stored the timestamp of the `last_anomaly` above for exactly this purpose."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "is_anomaly, run_time = detect_ts_online(df_smooth, 24*7*2, last_anomaly) \n\nprint('Anomaly: %s, run_time: %ss' % (is_anomaly, round(run_time,2)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Hands-on lab\n\nWe need to systematically explore how different window sizes affect our ability to detect anomalies in the time series.  When we do this, we need to consider both our precision and recall in doing so.  Depending on the operational cost of false positives and negatives, we may have to use different F-measures.\n\nIn order to do this, we define a function that samples machines and sensors from the data frame.  It checks whether the batch anomaly detection algorithm detected an anomaly at that time point, and tests whether the online anomaly detection algorithm comes to the same result. \n\nIt records the number of correct and incorrect online anomaly detection results, to calculate the accuracy of the online anomaly detection solution.\n\nThe script accepts two input arguments:\n- window_size: how much historic data to retain\n- com: Specify decay in terms of center of mass for running avg\n\n\nWe already provided the source code for this function below. But we would like you to discuss with your neighbors which of the following metrics you should use (follow the links to read up on the definition of these):\n- [precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)\n- [recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)\n- [f1-score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)\n- [fbeta-score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html) (which beta?)\n\nThink about the cost and savings of different settings for the size of the sliding window.  What if the cost of a false negative is high (e.g. USD 50000 for a broken machine), what if that cost is low (e.g. USD 10 for replacing a broken part and restarting the machine)?  What if a false positive is really expensive (e.g. USD 1000 for production loss while machine is being restarted)?\n\nTo conclude the lab, enter your choice at the bottom of the next script and run it."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import fbeta_score\nimport time\n\nfrom pyculiarity import detect_ts\n\ndef sample_run(df, anoms_ref, window_size = 500, com = 12, n_epochs=10):\n    \"\"\"\n    This functions expects a dataframe df as mandatory argument.  \n    The first column of the df should contain timestamps, the second machine IDs\n    \n    arguments:\n    df: a pandas data frame with two columns: 1. timestamp, 2. value\n    anoms_ref: reference anomaly detection results \n    \n    Keyword arguments:\n    window_size: the size of the window of data points that are used for anomaly detection\n    com: decay in terms of center of mass (this approximates averageing over about twice as many hours)\n    \"\"\"\n\n    p_anoms = .1\n\n    def detect_ts_online(df_smooth, window_size, stop):\n        is_anomaly = False\n        run_time = 9999\n        start_index = max(0, stop - window_size)\n        df_win = df_smooth.iloc[start_index:stop, :]\n        start_time = time.time()\n        results = detect_ts(df_win, alpha=0.05, max_anoms=0.02, only_last=None, longterm=False, e_value=False, direction='both')\n        run_time = time.time() - start_time\n        if results['anoms'].shape[0] > 0:\n            timestamp = df_win['timestamp'].tail(1).values[0]\n            if timestamp == results['anoms'].tail(1)['timestamp'].values[0]:\n                is_anomaly = True\n        return is_anomaly, run_time\n\n    def running_avg(ts, com=6):\n        rm_o = np.zeros_like(ts)\n        rm_o[0] = ts[0]\n    \n        for r in range(1, len(ts)):\n            curr_com = float(min(com, r))\n            rm_o[r] = rm_o[r-1] + (ts[r] - rm_o[r-1])/(curr_com + 1)\n    \n        return rm_o\n\n    # create arrays that will hold the results of batch AD (y_true) and online AD (y_pred)\n    y_true = [False] * n_epochs\n    y_pred = [True] * n_epochs\n    run_times = []\n    \n    # check which unique machines, sensors, and timestamps we have in the dataset\n    machineIDs = df['machineID'].unique()\n    sensors = df.columns[2:]\n    timestamps = df['datetime'].unique()[window_size:]\n    \n    # sample n_machines_test random machines and sensors \n    random_machines = np.random.choice(machineIDs, n_epochs)\n    random_sensors = np.random.choice(sensors, n_epochs)\n\n    # we intialize an array with that will later hold a sample of timetamps\n    random_timestamps = np.random.choice(timestamps, n_epochs)\n    \n    for i in range(0, n_epochs):\n        # take a slice of the dataframe that only contains the measures of one random machine\n        df_s = df[df['machineID'] == random_machines[i]]\n        \n        # smooth the values of one random sensor, using our running_avg function\n        smooth_values = running_avg(df_s[random_sensors[i]].values, com)\n        \n        # create a data frame with two columns: timestamp, and smoothed values\n        df_smooth = pd.DataFrame(data={'timestamp': df_s['datetime'].values, 'value': smooth_values})\n\n        # load the results of batch AD for this machine and sensor\n        anoms_s = anoms_ref[((anoms_ref['machineID'] == random_machines[i]) & (anoms_ref['errorID'] == random_sensors[i]))]\n                \n        # find the location of the t'th random timestamp in the data frame\n        if np.random.random() < p_anoms:\n            anoms_timestamps = anoms_s['datetime'].values\n            np.random.shuffle(anoms_timestamps)\n            counter = 0\n            while anoms_timestamps[0] < timestamps[0]:\n                if counter > 100:\n                    return 0.0, 9999.0\n                np.random.shuffle(anoms_timestamps)\n                counter += 1\n            random_timestamps[i] = anoms_timestamps[0]\n            \n        # select the test case\n        test_case = df_smooth[df_smooth['timestamp'] == random_timestamps[i]]\n        test_case_index = test_case.index.values[0]\n\n\n        # check whether the batch AD found an anomaly at that time stamps and copy into y_true at idx\n        y_true_i = random_timestamps[i] in anoms_s['datetime'].values\n\n        # perform online AD, and write result to y_pred\n        y_pred_i, run_times_i = detect_ts_online(df_smooth, window_size, test_case_index)\n        \n        y_true[i] = y_true_i\n        y_pred[i] = y_pred_i\n        run_times.append(run_times_i)\n            \n    return <your solution goes here>",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# %load ../solutions/sample_run.py",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Explore the parameter space:\n- Different settings for the size of the sliding window (e.g. 50, 100, 5000, 1000).\n- Different settings for the decay in terms of center of mass for running avg (e.g. 4,6,8,12 24)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import fbeta_score \n\nn_epochs = 10 # sample size\nwindow_size = 500\ncom = 12\n\nfscore, run_time = sample_run(telemetry, anoms_batch, window_size=window_size, com=com, n_epochs=n_epochs)\n\nprint(\"Fbeta-score: %s\" % fscore)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "If you ran the cell above repeatedely, you may have noticed that you can not get reliable results.  This is because we need to draw a bigger sample (n_epochs).  Unfortunately, the run-time of `sample_run` increases linearly with `n_epochs`.  In the next lab, we will therefore move on to run this on the cloud, using the Azure tools for Machine Learning experimentation. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "\n### End of lab\n\n## The end"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Copyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT License."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "name": "AD_model_development",
    "notebookId": 4063271094430170
  },
  "nbformat": 4,
  "nbformat_minor": 1
}